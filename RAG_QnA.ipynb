{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahma-Farag/langchain_project/blob/main/RAG_QnA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers transformers chromadb accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap datasets langchain-community ragatouille"
      ],
      "metadata": {
        "id": "fSBIr1n-KYV5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# userdata.get('HF_API')"
      ],
      "metadata": {
        "id": "au8OWjU6FBF6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_API')"
      ],
      "metadata": {
        "id": "T8hjdoX5FSMJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mdeJRni3FvAT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    # Get the device ID of the current GPU\n",
        "    device_id = torch.cuda.current_device()\n",
        "    print(f\"Current GPU device ID: {device_id}\")\n",
        "else:\n",
        "    print(\"CUDA (GPU) is not available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcCelwAN7P3o",
        "outputId": "c071f0e4-ec33-403f-a917-ce6dd09de353"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current GPU device ID: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # !pip install -q langchain==0.0.219\n",
        "# # !pip install langchain-huggingface\n",
        "# !pip install -q chromadb==0.3.26\n",
        "# !pip install -q transformers==4.29.2 sentencepiece==0.1.99 accelerate==0.19.0 bitsandbytes==0.39.0\n",
        "\n",
        "# !pip install -q python-dotenv==1.0.0\n",
        "\n",
        "# !pip install -q pandas==1.5.3\n",
        "# !pip install -q unstructured==0.7.12\n",
        "# !pip install -q wikipedia==1.4.0\n",
        "# !pip install -q pypdf==3.12.0\n",
        "# !pip install -q jq==1.4.1\n",
        "# !pip install -q nltk==3.8.1\n",
        "# !pip install -q tiktoken==0.4.0\n",
        "# !pip install -q transformers==4.39.0 sentencepiece==0.1.99\n",
        "# !pip install -q sentence-transformers==2.2.2\n",
        "# !pip install -q cohere==4.11.2\n",
        "# !pip install -q faiss-cpu==1.7.4\n",
        "\n"
      ],
      "metadata": {
        "id": "bfc6tHUGwQmA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install chromadb"
      ],
      "metadata": {
        "id": "mJ43HhyTVwjk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llm"
      ],
      "metadata": {
        "id": "yCvuh88ixvEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install torch"
      ],
      "metadata": {
        "id": "PrixHUclrb5v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdZNbyO_87T5",
        "outputId": "2d9ed778-53e8-409c-84af-078dfac9072a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers==4.11.3  # Adjust version as needed"
      ],
      "metadata": {
        "id": "TyzdyFv_CMqw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow keras"
      ],
      "metadata": {
        "id": "uvT_MMFaCf_2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFacePipeline\n",
        "# from langchain import HuggingFaceHub\n",
        "from transformers import pipeline\n",
        "import transformers\n",
        "import torch\n",
        "# model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "# # Set Hugging Face API key\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_huggingface_api_key_here\""
      ],
      "metadata": {
        "id": "T9bFDw1t5oIf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFaceEndpoint\n",
        "# model_id = \"gpt2\"\n",
        "# # model_id = 'distilbert/distilgpt2'\n",
        "# # model_id = 'deepset/roberta-base-squad2'\n",
        "# # qa_pipeline = pipeline(\"text-generation\",  model=model_id)\n",
        "# llm = HuggingFaceEndpoint(\n",
        "#     repo_id=model_id, max_new_tokens=400, temperature=0.5, huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
        "#     model_kwargs={\"max_length\":2500}\n",
        "# )"
      ],
      "metadata": {
        "id": "lf7ZbJf7G4bw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")"
      ],
      "metadata": {
        "id": "9_sRUW7I0iYz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "9cfb5f8b-2ea4-4039-aa7d-dde78a0762cc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-914a5640e49a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbnb_4bit_compute_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREADER_MODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREADER_MODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3049\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3050\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3051\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;34m\"and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "qX2iDEB7Hl11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WikipediaLoader, OnlinePDFLoader, UnstructuredURLLoader\n",
        "\n",
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "\n",
        "\n",
        "# load document\n",
        "# query = \"Nikola Tesla\"\n",
        "\n",
        "# docs = WikipediaLoader(query=query,\n",
        "#                        load_max_docs=1,\n",
        "#                        doc_content_chars_max=20_000).load()\n",
        "\n",
        "# documents = [ docs[0].page_content ]\n",
        "# metadatas = [ {\"document\": query} ]\n",
        "\n",
        "\n",
        "# url = \"https://laraveldaily.com/wp-content/uploads/2020/04/laravel-tips-2020-04.pdf\"\n",
        "\n",
        "# llm_loader = OnlinePDFLoader(url)\n",
        "\n",
        "# pages = llm_loader.load_and_split()\n",
        "\n",
        "\n",
        "# urls = [\n",
        "#     \"https://www.apple.com/in/support/products/faqs.html\",\n",
        "#     \"https://www.apple.com/legal/sales-support/\",\n",
        "# ]\n",
        "# llm_loader = UnstructuredURLLoader(urls=urls)\n",
        "# llm_data = llm_loader.load()\n",
        "\n",
        "\n",
        "\n",
        "# splitter\n",
        "text_splitter = NLTKTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "# tokens_chunks = text_splitter.create_documents(\n",
        "#     documents,\n",
        "#     metadatas=metadatas\n",
        "# )\n",
        "\n",
        "\n",
        "# docs_text = [ chunk.page_content for chunk in tokens_chunks ]\n",
        "# docs_embeddings = embedding_llm.embed_documents(docs_text)\n",
        "# query_text = \"Can you list a number of Nikola Tesla's inventions?\"\n",
        "# query_embedding = embedding_llm.embed_query(query_text)\n",
        "\n",
        "# vector database\n",
        "# save_to_dir = \"/content/wiki_chroma_db\"\n",
        "# vector_db = Chroma.from_documents(\n",
        "#     tokens_chunks,\n",
        "#     embedding_llm,\n",
        "#     persist_directory=save_to_dir\n",
        "# )"
      ],
      "metadata": {
        "id": "ftUICZDq02x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install streamlit pyngrok"
      ],
      "metadata": {
        "id": "dayNl4jE2dn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "def get_text_chunks_langchain(text):\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "    docs = [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "1t7btZoY8lgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "YlO2F5rz-dUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # llm = hf\n",
        "\n",
        "# qna_template = \"\\n\".join([\n",
        "#     \"Answer the next question using the provided context.\",\n",
        "#     \"If the answer is not contained in the context, say 'NO ANSWER IS AVAILABLE'\",\n",
        "#     \"### Context:\",\n",
        "#     \"{context}\",\n",
        "#     \"\",\n",
        "#     \"### Question:\",\n",
        "#     \"{question}\",\n",
        "#     \"\",\n",
        "#     \"### Answer:\",\n",
        "# ])\n",
        "\n",
        "# qna_prompt = PromptTemplate(\n",
        "#     template=qna_template,\n",
        "#     input_variables=['context', 'question'],\n",
        "# )\n",
        "\n",
        "# combine_template = \"\\n\".join([\n",
        "#     \"Given intermediate contexts for a question, generate a final answer.\",\n",
        "#     \"If the answer is not contained in the intermediate contexts, say 'NO ANSWER IS AVAILABLE'\",\n",
        "#     \"### Summaries:\",\n",
        "#     \"{summaries}\",\n",
        "#     \"\",\n",
        "#     \"### Question:\",\n",
        "#     \"{question}\",\n",
        "#     \"\",\n",
        "#     \"### Final Answer:\",\n",
        "# ])\n",
        "\n",
        "# combine_prompt = PromptTemplate(\n",
        "#     template=combine_template,\n",
        "#     input_variables=['summaries', 'question'],\n",
        "# )\n",
        "\n",
        "# map_reduce_chain = load_qa_chain(llm, chain_type=\"map_reduce\",\n",
        "#                                  return_intermediate_steps=True,\n",
        "#                                  question_prompt=qna_prompt,\n",
        "#                                  combine_prompt=combine_prompt)"
      ],
      "metadata": {
        "id": "mAvpewSO-ieg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(docs, question):\n",
        "\n",
        "  doc_content = [ doc.page_content for doc in docs]\n",
        "  metadatas = [ {\"document\":i} for i in range(len(docs))]\n",
        "  # splitter\n",
        "  text_splitter = NLTKTextSplitter(chunk_size=60, chunk_overlap=5)\n",
        "  tokens_chunks = text_splitter.create_documents(\n",
        "      doc_content,\n",
        "      metadatas=metadatas\n",
        "  )\n",
        "\n",
        "  # embeddings\n",
        "  model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "  embedding_llm = SentenceTransformerEmbeddings(model_name=model_name)\n",
        "\n",
        "  # vector database\n",
        "  save_to_dir = \"/content/wiki_chroma_db\"\n",
        "  vector_db = Chroma.from_documents(\n",
        "      tokens_chunks,\n",
        "      embedding_llm,\n",
        "      persist_directory=save_to_dir\n",
        "  )\n",
        "\n",
        "  similar_docs = vector_db.similarity_search(question, k=1)\n",
        "\n",
        "\n",
        "  retrieved_docs_text = [doc.page_content for doc in similar_docs]  # We only need the text of the documents\n",
        "  context = \"\\nExtracted documents:\\n\"\n",
        "  context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)])\n",
        "\n",
        "  final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "  # Redact an answer\n",
        "  final_answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "  # final_answer = map_reduce_chain({\"input_documents\": similar_docs,\n",
        "  #                                 \"question\": question\n",
        "  #                                 }, return_only_outputs=True)\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "SEWB93Z0-WZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = get_text_chunks_langchain(\"\"\"\n",
        "Ferrari S.p.A. (/fəˈrɑːri/; Italian: [ferˈraːri]) is an Italian luxury sports car manufacturer based in Maranello. Founded in 1939 by Enzo Ferrari (1898–1988), the company built its first car in 1940, adopted its current name in 1945, and began to produce its current line of road cars in 1947. Ferrari became a public company in 1960, and from 1963 to 2014 it was a subsidiary of Fiat S.p.A. It was spun off from Fiat's successor entity, Fiat Chrysler Automobiles, in 2016. In 2024, the Wall Street Journal described the company as having been \"synonymous with opulence, meticulous craftsmanship and ridiculously fast cars for nearly a century\".[6]\n",
        "\n",
        "The company currently offers a large model range which includes several supercars, grand tourers, and one SUV. Many early Ferraris, dating to the 1950s and 1960s, count among the most expensive cars ever sold at auction. Owing to a combination of its cars, enthusiast culture, and successful licensing deals, in 2019 Ferrari was labelled the world's strongest brand by the financial consultancy Brand Finance.[7] As of May 2023, Ferrari is also one of the largest car manufacturers by market capitalisation, with a value of approximately US$52 billion.[8]\n",
        "\n",
        "Throughout its history, the company has been noted for its continued participation in racing, especially in Formula One, where its team, Scuderia Ferrari, is the series' single oldest and most successful. Scuderia Ferrari has raced since 1929, first in Grand Prix events and later in Formula One, where since 1952 it has fielded fifteen champion drivers, won sixteen Constructors' Championships, and accumulated more race victories, 1–2 finishes, podiums, pole positions, fastest laps and points than any other team in F1 history.[9][10] Historically, Ferrari was also highly active in sports car racing, where its cars took many wins in races such as the Mille Miglia, Targa Florio and 24 Hours of Le Mans, as well as several overall victories in the World Sportscar Championship. Scuderia Ferrari fans, commonly called tifosi, are known for their passion and loyalty to the team.\n",
        "\n",
        "History\n",
        "Main article: History of Ferrari\n",
        "Early history\n",
        "\n",
        "Three Scuderia Ferrari cars in 1934, all Alfa Romeo P3s. Drivers, left to right: Achille Varzi, Louis Chiron, and Carlo Felice Trossi.\n",
        "Enzo Ferrari, formerly a salesman and racing driver for Alfa Romeo, founded Scuderia Ferrari, a racing team, in 1929. Originally intended to service gentleman drivers and other amateur racers, Alfa Romeo's withdrawal from racing in 1933, combined with Enzo's connections within the company, turned Scuderia Ferrari into its unofficial representative on the track.[11] Alfa Romeo supplied racing cars to Ferrari, who eventually amassed some of the best drivers of the 1930s and won many races before the team's liquidation in 1937.[11][12]: 43\n",
        "\n",
        "Late in 1937, Scuderia Ferrari was liquidated and absorbed into Alfa Romeo,[11] but Enzo's disagreements with upper management caused him to leave in 1939. He used his settlement to found his own company, where he intended to produce his own cars. He called the company \"Auto Avio Costruzioni\", and headquartered it in the facilities of the old Scuderia Ferrari;[1] due to a noncompete agreement with Alfa Romeo, the company could not use the Ferrari name for another four years. The company produced a single car, the Auto Avio Costruzioni 815, which participated in only one race before the outbreak of World War II. During the war, Enzo's company produced aircraft engines and machine tools for the Italian military; the contracts for these goods were lucrative, and provided the new company with a great deal of capital. In 1943, under threat of Allied bombing raids, the company's factory was moved to Maranello. Though the new facility was nonetheless bombed twice, Ferrari remains in Maranello to this day.[1][12]: 45–47 [13]\n",
        "\n",
        "Under Enzo Ferrari\n",
        "\n",
        "Ferrari's factory in the early 1960s: everything in its production line was handmade by machinists, who followed technical drawings with extreme precision.[14] Much of this work is now done by industrial robots.[15]\n",
        "In 1945, Ferrari adopted its current name. Work started promptly on a new V12 engine that would power the 125 S, which was the marque's first car, and many subsequent Ferraris. The company saw success in motorsport almost as soon as it began racing: the 125 S won many races in 1947,[16][17] and several early victories, including the 1949 24 Hours of Le Mans and 1951 Carrera Panamericana, helped build Ferrari's reputation as a high-quality automaker.[18][19] Ferrari won several more races in the coming years,[9][20] and early in the 1950s its road cars were already a favourite of the international elite.[21] Ferrari produced many families of interrelated cars, including the America, Monza, and 250 series, and the company's first series-produced car was the 250 GT Coupé, beginning in 1958.[22]\n",
        "\n",
        "In 1960, Ferrari was reorganized as a public company. It soon began searching for a business partner to handle its manufacturing operations: it first approached Ford in 1963, though negotiations fell through; later talks with Fiat, who bought 50% of Ferrari's shares in 1969, were more successful.[23][24] In the second half of the decade, Ferrari also produced two cars that upended its more traditional models: the 1967 Dino 206 GT, which was its first mass-produced mid-engined road car,[a] and the 1968 365 GTB/4, which possessed streamlined styling that modernised Ferrari's design language.[27][28] The Dino in particular was a decisive movement away from the company's conservative engineering approach, where every road-going Ferrari featured a V12 engine placed in the front of the car, and it presaged Ferrari's full embrace of mid-engine architecture, as well as V6 and V8 engines, in the 1970s and 1980s.[27]\n",
        "\n",
        "Contemporary\n",
        "Enzo Ferrari died in 1988, an event that saw Fiat expand its stake to 90%.[29] The last car that he personally approved—the F40—expanded on the flagship supercar approach first tried by the 288 GTO four years earlier.[30] Enzo was replaced in 1991 by Luca Cordero di Montezemolo, under whose 23-year-long chairmanship the company greatly expanded. Between 1991 and 2014, he increased the profitability of Ferrari's road cars nearly tenfold, both by increasing the range of cars offered and through limiting the total number produced. Montezemolo's chairmanship also saw an expansion in licensing deals, a drastic improvement in Ferrari's Formula One performance (not least through the hiring of Michael Schumacher and Jean Todt), and the production of three more flagship cars: the F50, the Enzo, and the LaFerrari. In addition to his leadership of Ferrari, Montezemolo was also the chairman of Fiat proper between 2004 and 2010.[31]\n",
        "\n",
        "After Montezemolo resigned, he was replaced in quick succession by many new chairmen and CEOs. He was succeeded first by Sergio Marchionne,[31] who would oversee Ferrari's initial public offering and subsequent spin-off from Fiat Chrysler Automobiles,[32][33] and then by Louis Camilleri as CEO and John Elkann as chairman.[34] Beginning in 2021, Camilleri was replaced as CEO by Benedetto Vigna, who has announced plans to develop Ferrari's first fully electric model.[35] During this period, Ferrari has expanded its production, owing to a global increase in wealth, while becoming more selective with its licensing deals.[36][37]\n",
        "\n",
        "Motorsport\n",
        "Main article: Scuderia Ferrari\n",
        "For a complete list of Ferrari racing cars, see List of Ferrari competition cars.\n",
        "Since the company's beginnings, Ferrari has been involved in motorsport. Through its works team, Scuderia Ferrari, it has competed in a range of categories including Formula One and sports car racing, though the company has also worked in partnership with other teams.\n",
        "\n",
        "Grand Prix and Formula One racing\n",
        "Further information: Grand Prix racing history of Scuderia Ferrari and Ferrari Grand Prix results\n",
        "\n",
        "A Ferrari F2004 Formula One car, driven by Michael Schumacher. Schumacher is one of the most decorated drivers in F1 history.\n",
        "The earliest Ferrari entity, Scuderia Ferrari, was created in 1929—ten years before the founding of Ferrari proper—as a Grand Prix racing team. It was affiliated with automaker Alfa Romeo, for whom Enzo had worked in the 1920s. Alfa Romeo supplied racing cars to Ferrari, which the team then tuned and adjusted to their desired specifications. Scuderia Ferrari was highly successful in the 1930s: between 1929 and 1937 the team fielded such top drivers as Antonio Ascari, Giuseppe Campari, and Tazio Nuvolari, and won 144 out of its 225 races.[12][11]\n",
        "\n",
        "Ferrari returned to Grand Prix racing in 1947, which was at that point metamorphosing into modern-day Formula One. The team's first homebuilt Grand Prix car, the 125 F1, was first raced at the 1948 Italian Grand Prix, where its encouraging performance convinced Enzo to continue the company's costly Grand Prix racing programme.[38]: 9  Ferrari's first victory in an F1 series was at the 1951 British Grand Prix, heralding its strong performance during the 1950s and early 1960s: between 1952 and 1964, the team took home six World Drivers' Championships and one Constructors' Championship. Notable Ferrari drivers from this era include Alberto Ascari, Juan Manuel Fangio, Phil Hill, and John Surtees.[9]\n",
        "\n",
        "Ferrari's initial fortunes ran dry after 1964, and its began to receive its titles in isolated sprees.[10] Ferrari first started to slip in the late 1960s, when it was outclassed by teams using the inexpensive, well-engineered Cosworth DFV engine.[39][40] The team's performance improved markedly in the mid-1970s thanks to Niki Lauda, whose skill behind the wheel granted Ferrari a drivers' title in 1975 and 1977; similar success was accomplished in following years by the likes of Jody Scheckter and Gilles Villeneuve.[10][41] The team also won the Constructors' Championship in 1982 and 1983.[9][42]\n",
        "\n",
        "Following another drought in the 1980s and 1990s, Ferrari saw a long winning streak in the 2000s, largely through the work of Michael Schumacher. After signing onto the team in 1996, Schumacher gave Ferrari five consecutive drivers' titles between 2000 and 2004; this was accompanied by six consecutive constructors' titles, beginning in 1999. Ferrari was especially dominant in the 2004 season, where it lost only three races.[9] After Schumacher's departure, Ferrari won one more drivers' title—given in 2007 to Kimi Räikkönen—and two constructors' titles in 2007 and 2008. These are the team's most recent titles to date; as of late, Ferrari has struggled to outdo recently ascendant teams such as Red Bull and Mercedes-Benz.[9][10]\n",
        "\n",
        "Ferrari Driver Academy\n",
        "Main article: Ferrari Driver Academy\n",
        "Ferrari's junior driver programme is the Ferrari Driver Academy. Begun in 2009, the initiative follows the team's successful grooming of Felipe Massa between 2003 and 2006. Drivers who are accepted into the Academy learn the rules and history of formula racing as they compete, with Ferrari's support, in feeder classes such as Formula Three and Formula 4.[43][44][45] As of 2019, 5 out of 18 programme inductees had graduated and become F1 drivers: one of these drivers, Charles Leclerc, came to race for Scuderia Ferrari, while the other four signed to other teams. Non-graduate drivers have participated in racing development, filled consultant roles, or left the Academy to continue racing in lower-tier formulae.[45]\n",
        "\n",
        "Sports car racing\n",
        "\n",
        "A 312 P, driven by Jacky Ickx, during Ferrari's final year in the World Sportscar Championship\n",
        "Aside from an abortive effort in 1940, Ferrari began racing sports cars in 1947, when the 125 S won six out of the ten races it participated in. [16] Ferrari continued to see similar luck in the years to follow: by 1957, just ten years after beginning to compete, Ferrari had won three World Sportscar Championships, seven victories in the Mille Miglia, and two victories at the 24 Hours of Le Mans, among many other races[20] These races were ideal environments for the development and promotion of Ferrari's earlier road cars, which were broadly similar to their racing counterparts.[46]\n",
        "\n",
        "This luck continued into the first half of the 1960s, when Ferrari won the WSC's 2000GT class three consecutive times and finished first at Le Mans for six consecutive years.[47][48] Its winning streak at Le Mans was broken by Ford in 1966,[48] and though Ferrari would win two more WSC titles—one in 1967 and another in 1972[49][50]—poor revenue allocation, combined with languishing performance in Formula One, led the company to cease competing in sports car events in 1973.[24]: 621  From that point onward, Ferrari would help prepare sports racing cars for privateer teams, but would not race them itself.[51]\n",
        "\n",
        "In 2023, Ferrari reentered sports car racing. For the 2023 FIA World Endurance Championship, Ferrari, in partnership with AF Corse, fielded two 499P sports prototypes. To commemorate the company's return to the discipline, one of the cars was numbered \"50\", referencing the fifty years that had elapsed since a works Ferrari competed in an endurance race.[52][53] The 499P finished first at the 2023 24 Hours of Le Mans, ending Toyota Gazoo Racing's six-year winning streak there and becoming the first Ferrari in 58 years to win the race.[54]\n",
        "\n",
        "Other disciplines\n",
        "From 1932 to 1935 Scuderia Ferrari operated a motorcycle racing division, which was conceived as a way to scout and train future Grand Prix drivers. Instead of Italian motorcycles, the team used British ones manufactured by Norton and Rudge. Though Ferrari was successful on two wheels, winning three national titles and 44 overall victories, it was eventually pushed out of the discipline both by the obsolescence of pushrod motorcycle engines and broader economic troubles stemming from the Great Depression.[55][56]\n",
        "\n",
        "Ferrari formerly participated in a variety of non-F1 open-wheel series. As early as 1948, Ferrari had developed cars for Formula Two and Formula Libre events,[57] and the company's F2 programme led directly to the creation of the Dino engine, which came to power various racing and road Ferraris.[27] The final non-F1 formula in which Ferrari competed was the Tasman Series, wherein Chris Amon won the 1969 championship in a Dino 246 Tasmania.[58]\n",
        "\n",
        "At least two water speed record boats have utilized Ferrari powertrains, both of them 800kg-class hydroplanes from the early 1950s. Neither boat was built by or affiliated with Ferrari, though one of them, Arno XI, had its engine order approved directly by Enzo Ferrari. Arno XI still holds the top speed record for an 800kg hydroplane\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "6yxAsZowNca1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer(docs, \"when was it the most popular?\")"
      ],
      "metadata": {
        "id": "C777FdlSNXjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from io import StringIO\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Set page title and layout\n",
        "st.set_page_config(page_title=\"Q&A App\", layout=\"wide\")\n",
        "\n",
        "# Title of the app\n",
        "st.title(\"Document & URL Q&A App\")\n",
        "\n",
        "# Sidebar for document upload and URL input\n",
        "st.sidebar.header(\"Upload Document or Enter URL\")\n",
        "\n",
        "# Option to upload a document\n",
        "uploaded_file = st.sidebar.file_uploader(\"Choose a document...\", type=[\"txt\", \"pdf\"])\n",
        "\n",
        "# Option to input a URL\n",
        "url = st.sidebar.text_input(\"Enter a URL\")\n",
        "\n",
        "# Display the uploaded document or URL\n",
        "document_content = None\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    st.subheader(\"Uploaded Document\")\n",
        "    if uploaded_file.type == \"text/plain\":\n",
        "        stringio = StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n",
        "        document_content = stringio.read()\n",
        "        st.text_area(\"Document content:\", document_content, height=300)\n",
        "\n",
        "        docs = get_text_chunks_langchain(document_content)\n",
        "    elif uploaded_file.type == \"application/pdf\":\n",
        "        # Read PDF file\n",
        "        pdf_document = fitz.open(stream=uploaded_file.read(), filetype=\"pdf\")\n",
        "        document_content = \"\"\n",
        "        for page_num in range(pdf_document.page_count):\n",
        "            page = pdf_document.load_page(page_num)\n",
        "            document_content += page.get_text()\n",
        "        st.text_area(\"Document content:\", document_content, height=300)\n",
        "\n",
        "        docs = get_text_chunks_langchain(document_content)\n",
        "    else:\n",
        "        st.warning(\"Only .txt and .pdf files are supported at this time.\")\n",
        "elif url:\n",
        "    st.subheader(\"Entered URL\")\n",
        "    st.write(f\"URL: {url}\")\n",
        "\n",
        "    llm_loader = UnstructuredURLLoader(urls=[url])\n",
        "    docs = llm_loader.load()\n",
        "\n",
        "else:\n",
        "  docs = None\n",
        "\n",
        "# Input for user's question\n",
        "st.subheader(\"Ask a Question\")\n",
        "question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "# Placeholder for the answer\n",
        "if question:\n",
        "    st.subheader(\"Answer\")\n",
        "    # Placeholder where you will handle the answer logic\n",
        "    if docs != None:\n",
        "      answer = get_answer(docs)\n",
        "      st.write(\"This is where the answer will be displayed.\")\n",
        "\n",
        "# Instructions or notes\n",
        "st.sidebar.info(\n",
        "    \"\"\"\n",
        "    Please upload a document or enter a URL for the content you want to ask questions about.\n",
        "    Then, enter your question in the main area.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Add a footer\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "        footer {\n",
        "            visibility: hidden;\n",
        "        }\n",
        "        footer:after {\n",
        "            content:'Q&A App by YourName';\n",
        "            visibility: visible;\n",
        "            display: block;\n",
        "            position: relative;\n",
        "            padding: 10px;\n",
        "            top: 2px;\n",
        "        }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "id": "giviXMGd2eMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Start the Streamlit app\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
        "\n",
        "# Allow some time for the app to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f'Public URL: {public_url}')\n"
      ],
      "metadata": {
        "id": "ImyAPuY32iSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mIoxpeh3fLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}